{"cells":[{"cell_type":"markdown","metadata":{"id":"kRXA63pgF_5J"},"source":["# Experiments of the paper: \"Efficient Detection of SQL Injection Attacks Utilising Cascade Natural Language Processing\""]},{"cell_type":"markdown","metadata":{"id":"mlW1ajN9GOp_"},"source":["## Initializations"]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"MqgCOCzc1AFe","executionInfo":{"status":"ok","timestamp":1681594290255,"user_tz":-60,"elapsed":5,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1697,"status":"ok","timestamp":1681594078038,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"r3SajLBfMY2f","outputId":"1b3ea306-80c7-4ace-9496-0247485c87a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":204,"status":"ok","timestamp":1681594295690,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"newbm9LK79G_"},"outputs":[],"source":["from pathlib import Path\n","main_folder = Path('/content/drive/MyDrive/Akademik/Research and Projects/Sakir Hoca Projects/AI Security Intelligence/Codes/20230415_sqli_colab')\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":318,"status":"ok","timestamp":1681594296775,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"w_-R9cC9lWx8","outputId":"2cbbfda1-3a2c-43ca-c5c2-660504015237"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Akademik/Research and Projects/Sakir Hoca Projects/AI Security Intelligence/Codes/20230415_sqli_colab\n","total 133K\n","-rw------- 1 root root  683 Mar 31 15:15 classical_models.py\n","-rw------- 1 root root  587 Apr  9 12:15 config.ini\n","drwx------ 2 root root 4.0K Apr  9 12:12 \u001b[0m\u001b[01;34mdatasets\u001b[0m/\n","-rw------- 1 root root 7.6K Mar 31 15:15 ensemble_models.py\n","-rw------- 1 root root  13K Apr 10 16:42 experiments.py\n","-rw------- 1 root root  739 Mar 31 15:15 .gitignore\n","-rw------- 1 root root    3 Jan 19 13:28 LICENSE.md\n","-rw------- 1 root root  77K Apr 15 21:31 main.ipynb\n","drwx------ 2 root root 4.0K Apr 15 13:41 \u001b[01;34m__pycache__\u001b[0m/\n","-rw------- 1 root root 5.0K Apr  4 22:08 README.md\n","drwx------ 2 root root 4.0K Apr 15 20:54 \u001b[01;34mresults\u001b[0m/\n","-rw------- 1 root root 1.3K Apr  4 22:08 run_classical_MLs.py\n","-rw------- 1 root root 5.8K Apr  4 22:08 templates.py\n","drwx------ 2 root root 4.0K Apr  9 12:12 \u001b[01;34mtrained_models\u001b[0m/\n","drwx------ 2 root root 4.0K Apr 10 22:05 \u001b[01;34mutils\u001b[0m/\n"]}],"source":["%cd $main_folder\n","%ls -lah"]},{"cell_type":"markdown","metadata":{"id":"k0DXUQ1tIqEF"},"source":["Warning: BERT libraries had problems after Colab system is upgraded. The solution I found is to downgrade numpy. After running the following code, restart the session and check the numpy version. If the libraries are working just fine in future, you can remove the downgrade fix."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6309,"status":"ok","timestamp":1681594096287,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"Xmn1CHABCsZi","outputId":"b8ff4a6a-749e-4bf3-889a-f3a7dcc51c6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-model-optimization 0.7.4 requires numpy~=1.23, but you have numpy 1.22.0 which is incompatible.\n","librosa 0.10.0.post2 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you have numpy 1.22.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q -U numpy==1.22 --ignore-installed"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1681594300182,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"tqD40wf4069j","outputId":"88873553-c9ff-4dae-c327-05e4e3e3298d"},"outputs":[{"output_type":"stream","name":"stdout","text":["numpy: \t\t 1.22.0\n","sklearn: \t 1.2.2\n","Python 3.9.16\n"]}],"source":["import numpy as np\n","import sklearn\n","print(\"numpy: \\t\\t\", np.__version__)\n","print(\"sklearn: \\t\", sklearn.__version__)\n","!python --version"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4146,"status":"ok","timestamp":1681594308817,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"q-YbjCkzw0yU"},"outputs":[],"source":["# A dependency of the preprocessing for BERT inputs\n","#!pip install  -q -U \"tensorflow-text==2.8.*\"\n","!pip install  -q -U \"tensorflow-text\""]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4178,"status":"ok","timestamp":1681594312990,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"b-P1ZOA0FkVJ"},"outputs":[],"source":["#!pip install -q tf-models-official==2.7.0\n","!pip install -q tf-models-official"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1681594312990,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"Fw2_o8VS_hpZ","outputId":"e71f76cd-b22d-476d-c481-235db8d2b296"},"outputs":[{"output_type":"stream","name":"stdout","text":["The scikit-learn version is 1.2.2.\n"]}],"source":["import sklearn\n","print('The scikit-learn version is {}.'.format(sklearn.__version__))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3528,"status":"ok","timestamp":1681594316513,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"b9AprPGFMhQ4","outputId":"0a09f9ce-0d07-408a-dc62-322907a87847"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Pandas v 1.5.3\n"]}],"source":["from pathlib import Path\n","from datetime import datetime\n","import os\n","import shutil\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from official.nlp import optimization  # to create AdamW optimizer\n","\n","import matplotlib.pyplot as plt\n","\n","import pandas as pd\n","import time\n","\n","\n","import time\n","from sklearn import metrics\n","from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n","\n","from experiments import evaluate, save_results\n","\n","print('Pandas v', pd.__version__)\n","#tf.get_logger().setLevel('ERROR')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1681594316855,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"iXfCnfKRZMty","outputId":"c5e11ef7-9d30-4dd4-9247-3b637cd69dc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Apr 15 21:31:56 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   30C    P0    42W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":[" !nvidia-smi"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1681594316855,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"t48CqXYRmrey","outputId":"bcef3212-1f1b-462c-b14c-47f0be7abe9e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['config.ini']"]},"metadata":{},"execution_count":14}],"source":["import configparser\n","from templates import DataManager, logger\n","config = configparser.ConfigParser()\n","config.read('config.ini')"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":316,"status":"ok","timestamp":1681594317665,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"nseZXgKhpjjh"},"outputs":[],"source":["data_manager = DataManager(config)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1681594321198,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"ccnJtym5lWx-","outputId":"15e81024-9eaf-47d5-8c61-4a7f1c3ecedc"},"outputs":[{"output_type":"stream","name":"stdout","text":["All results:/content/drive/MyDrive/Akademik/Research and Projects/Sakir Hoca Projects/AI Security Intelligence/Codes/20230415_sqli_colab/results/results_230415_213200.csv\n","Proposed method results:/content/drive/MyDrive/Akademik/Research and Projects/Sakir Hoca Projects/AI Security Intelligence/Codes/20230415_sqli_colab/results/proposed_method_results.csv\n"]}],"source":["#%%script echo skipping\n","results_dir = Path(config['results']['dir'])\n","\n","currentDateAndTime = datetime.now()\n","currentTime = currentDateAndTime.strftime(\"%y%m%d_%H%M%S\")\n","file_name = f'results_{currentTime}.csv'\n","output_file = main_folder / results_dir / file_name\n","proposed_test_results_file = main_folder / results_dir / 'proposed_method_results.csv'\n","print(f\"All results:{output_file.absolute()}\")\n","print(f\"Proposed method results:{proposed_test_results_file.absolute()}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tr6pW32WGi1D"},"source":["## Classical ML (Single NLP) tests"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323410,"status":"ok","timestamp":1681146025245,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"8RC2O_SyuIFe","outputId":"0a032945-54b9-41f5-a3cd-447c3773b8c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["04-10-2023 16:55:03 l:26| Loaded dataset with 30609 rows.\n","Running the tests for seed: 42\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","04-10-2023 16:55:03 l:140| Training model: xgboost\n","04-10-2023 16:55:04 l:148| Ended training xgboost in: 1.1999408399999538s\n","04-10-2023 16:55:04 l:140| Training model: naive_bayes\n","04-10-2023 16:55:04 l:148| Ended training naive_bayes in: 0.0055923019999681856s\n","04-10-2023 16:55:04 l:140| Training model: svm\n","04-10-2023 16:55:50 l:148| Ended training svm in: 46.16930955499993s\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","04-10-2023 16:55:55 l:140| Training model: xgboost\n","04-10-2023 16:56:03 l:148| Ended training xgboost in: 8.000112153999908s\n","04-10-2023 16:56:03 l:140| Training model: naive_bayes\n","04-10-2023 16:56:03 l:148| Ended training naive_bayes in: 0.016362807999939832s\n","04-10-2023 16:56:04 l:140| Training model: svm\n","04-10-2023 16:58:10 l:148| Ended training svm in: 126.61472226000001s\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","04-10-2023 16:58:22 l:140| Training model: xgboost\n","04-10-2023 16:58:25 l:148| Ended training xgboost in: 2.8176168910001707s\n","04-10-2023 16:58:25 l:140| Training model: naive_bayes\n","04-10-2023 16:58:25 l:148| Ended training naive_bayes in: 0.006293993999861414s\n","04-10-2023 16:58:25 l:140| Training model: svm\n","04-10-2023 16:58:31 l:148| Ended training svm in: 5.814339034000113s\n","04-10-2023 16:58:32 l:140| Training model: ensemble_1\n","04-10-2023 16:58:32 l:148| Ended training ensemble_1 in: 1.3241000033303862e-05s\n","04-10-2023 16:58:47 l:140| Training model: ensemble_2\n","04-10-2023 16:58:47 l:148| Ended training ensemble_2 in: 1.4787000054639066e-05s\n","04-10-2023 16:59:03 l:140| Training model: ensemble_4\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","04-10-2023 17:00:04 l:148| Ended training ensemble_4 in: 61.46812302500007s\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","04-10-2023 17:00:15 l:140| Training model: xgboost\n","04-10-2023 17:00:23 l:148| Ended training xgboost in: 8.394619485000021s\n","Prediction probabilities are saved to results/xgboost_pred_230410_170023.csv\n","header is True\n","04-10-2023 17:00:23 l:179| Results saved to results/results_230410_165455.csv\n"]}],"source":["!python run_classical_MLs.py -o $file_name"]},{"cell_type":"markdown","metadata":{"id":"8VzVtm9TGuOC"},"source":["## LLM (BERT) tests"]},{"cell_type":"code","execution_count":17,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1681594333689,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"y8_ctG55-uTX","outputId":"aa60d4ab-e732-4bab-c70d-961a042a8e83"},"outputs":[{"output_type":"stream","name":"stdout","text":["BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"]}],"source":["#@title Choose a BERT model to fine-tune\n","\n","bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n","\n","map_name_to_handle = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_base/2',\n","    'electra_small':\n","        'https://tfhub.dev/google/electra_small/2',\n","    'electra_base':\n","        'https://tfhub.dev/google/electra_base/2',\n","    'experts_pubmed':\n","        'https://tfhub.dev/google/experts/bert/pubmed/2',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n","}\n","\n","map_model_to_preprocess = {\n","    'bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_en_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'bert_multi_cased_L-12_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n","    'albert_en_base':\n","        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n","    'electra_small':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'electra_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_pubmed':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'experts_wiki_books':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","    'talking-heads_base':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","}\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5xwsuzFaKiL"},"outputs":[],"source":["# Manual selection\n","bert_model_name_list = ['bert_en_uncased_L-12_H-768_A-12', \n","                        'bert_en_cased_L-12_H-768_A-12',\n","                        'small_bert/bert_en_uncased_L-2_H-128_A-2',\n","                        'small_bert/bert_en_uncased_L-4_H-512_A-8',\n","                        'small_bert/bert_en_uncased_L-8_H-128_A-2',\n","                        'small_bert/bert_en_uncased_L-12_H-768_A-12',\n","                        'bert_multi_cased_L-12_H-768_A-12',\n","                       'albert_en_base',\n","                        'electra_base',\n","                        'electra_small']"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":226,"status":"ok","timestamp":1681594338972,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"aksj743St9ga"},"outputs":[],"source":["def build_classifier_model(local_tfhub_handle_preprocess, local_tfhub_handle_encoder):\n","  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='Payload')\n","  preprocessing_layer = hub.KerasLayer(local_tfhub_handle_preprocess, name='preprocessing')\n","  encoder_inputs = preprocessing_layer(text_input)\n","  encoder = hub.KerasLayer(local_tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","  outputs = encoder(encoder_inputs)\n","  net = outputs['pooled_output']\n","  net = tf.keras.layers.Dropout(0.1)(net)\n","  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n","  return tf.keras.Model(text_input, net)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1681146025718,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"xFHT4mtCsKHP","outputId":"5b8dbae8-253f-40ee-fb10-7bbf4ce77851"},"outputs":[{"data":{"text/plain":["24487"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["data_manager.x_train.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tsOsmvENQSG"},"outputs":[],"source":["#main_dir = Path('/content/drive/MyDrive/Colab Notebooks/kasim/2023 Feb Bert SQLi/')\n","# 'SQLiV3_train.tsv', 'SQLiV3_test.tsv'\n","# 'NEW_normal_plus_waf_bypass_dataset_testing.tsv'\n","# 'NEW_normal_plus_waf_bypass_dataset_training.tsv'\n","#train_file_name = 'NEW_normal_plus_waf_bypass_dataset_training.tsv'\n","#train_file_name = 'SQLiV3_train.tsv'\n","#test_file_name = 'SQLiV3_test.tsv'\n","#train_file = Path(main_dir / train_file_name)\n","#test_file = Path(main_dir / test_file_name)\n","\n","#print(f'Train file exists: {train_file.is_file()}')\n","#print(f'Test file exists: {test_file.is_file()}')\n","\n","\n","currentDateAndTime = datetime.now()\n","currentTime = currentDateAndTime.strftime(\"%y%m%d_%H%M%S\")\n","#recording = Recorder(main_folder / results_dir / f'results_BERT_{currentTime}.csv')\n","\n","\n","AUTOTUNE = tf.data.AUTOTUNE\n","batch_size = 16\n","#seed = 42\n","\n","x_train = data_manager.x_train\n","y_train = data_manager.y_train\n","x_test = data_manager.x_test\n","y_test = data_manager.y_test\n","\n","# Create TensorFlow datasets for the training and validation sets\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(8)\n","val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(8)\n","\n","# Cache the training and validation datasets\n","train_dataset = train_dataset.cache()\n","val_dataset = val_dataset.cache()\n","\n","\n","# Prefetch the training and validation datasets\n","train_ds = train_dataset.prefetch(buffer_size=AUTOTUNE)\n","test_ds = val_dataset.prefetch(buffer_size=AUTOTUNE)\n","\n","\n","\n","#train_ds = test_csv_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","#val_ds = test_csv_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","# test_ds = val_ds\n","\n","class_names = ['payload','label']\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1681146028488,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"KT17nxwbHb9u","outputId":"09fc335f-2348-4dab-df61-582310a95925"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[b'if  (  6554  =  1534  )   select 6554 else drop function buqb--'\n"," b\"admin'  )   or   (  '1'  =  '1'--\"\n"," b' select * from users where id  =  1 or \"&  )  \" or 1  =  1 -- 1'\n"," b'callejon sur, 93 13-c' b\"-6449'   )    )    or 2590  =  2848\"\n"," b\"1' in boolean mode  ) \"\n"," b\"1' and 5556  =    (  select count  (  *  )   from all_users t1,all_users t2,all_users t3,all_users t4,all_users t5  )  \"\n"," b'SELECT MIN ( wood )  AS union FROM disease'], shape=(8,), dtype=string)\n","tf.Tensor([1 1 1 0 1 1 1 0], shape=(8,), dtype=int64)\n"]}],"source":["tmp = train_ds.take(1)\n","for i in next(iter(tmp)):\n","  print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"6QVYvfmcwWgQ","outputId":"060953fb-32e4-4962-9ec4-0c880ce297e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Epoch 1/10\n","3061/3061 [==============================] - 189s 56ms/step - loss: 0.1124 - binary_accuracy: 0.9591 - val_loss: 0.0236 - val_binary_accuracy: 0.9967\n","Epoch 2/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0151 - binary_accuracy: 0.9972 - val_loss: 0.0171 - val_binary_accuracy: 0.9980\n","Epoch 3/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0092 - binary_accuracy: 0.9984 - val_loss: 0.0179 - val_binary_accuracy: 0.9977\n","Epoch 4/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0064 - binary_accuracy: 0.9989 - val_loss: 0.0166 - val_binary_accuracy: 0.9982\n","Epoch 5/10\n","3061/3061 [==============================] - 174s 57ms/step - loss: 0.0087 - binary_accuracy: 0.9985 - val_loss: 0.0184 - val_binary_accuracy: 0.9980\n","Epoch 6/10\n","3061/3061 [==============================] - 174s 57ms/step - loss: 0.0057 - binary_accuracy: 0.9991 - val_loss: 0.0221 - val_binary_accuracy: 0.9982\n","Epoch 7/10\n","3061/3061 [==============================] - 173s 56ms/step - loss: 0.0075 - binary_accuracy: 0.9986 - val_loss: 0.0204 - val_binary_accuracy: 0.9979\n","Epoch 8/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0062 - binary_accuracy: 0.9991 - val_loss: 0.0209 - val_binary_accuracy: 0.9980\n","Epoch 9/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0068 - binary_accuracy: 0.9987 - val_loss: 0.0090 - val_binary_accuracy: 0.9989\n","Epoch 10/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0049 - binary_accuracy: 0.9992 - val_loss: 0.0119 - val_binary_accuracy: 0.9985\n","766/766 [==============================] - 13s 17ms/step - loss: 0.0119 - binary_accuracy: 0.9985\n","Loss: 0.011930594220757484\n","Accuracy: 0.9985299110412598\n","766/766 [==============================] - 12s 15ms/step\n","{'accuracy': 0.9978765109441359, 'precision': 0.9964850615114236, 'recall': 0.9978002639683238, 'f1_score': 0.9971422290613322, 'tp': 2268, 'tn': 3841, 'fp': 8, 'fn': 5, 'feature_method': 'bert_en_uncased_L-12_H-768_A-12', 'model': 'bert_en_uncased_L-12_H-768_A-12', 'seed': 42, 'split_ratio': 0.2, 'train_size': 24487, 'test_size': 6122, 'extraction_time': 0, 'feature_size': 0, 'train_time': 71.06877240816453, 'pred_time': 2.1081641778880496, 'dataset': 'SQLiV3.tsv'}\n","Appending to the existing .csv file.\n","****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3\n","Epoch 1/10\n","3061/3061 [==============================] - 188s 56ms/step - loss: 0.1493 - binary_accuracy: 0.9354 - val_loss: 0.0191 - val_binary_accuracy: 0.9974\n","Epoch 2/10\n","3061/3061 [==============================] - 170s 56ms/step - loss: 0.0125 - binary_accuracy: 0.9977 - val_loss: 0.0209 - val_binary_accuracy: 0.9974\n","Epoch 3/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0089 - binary_accuracy: 0.9985 - val_loss: 0.0122 - val_binary_accuracy: 0.9985\n","Epoch 4/10\n","3061/3061 [==============================] - 173s 57ms/step - loss: 0.0048 - binary_accuracy: 0.9991 - val_loss: 0.0161 - val_binary_accuracy: 0.9987\n","Epoch 5/10\n","3061/3061 [==============================] - 173s 56ms/step - loss: 0.0047 - binary_accuracy: 0.9992 - val_loss: 0.0108 - val_binary_accuracy: 0.9989\n","Epoch 6/10\n","3061/3061 [==============================] - 173s 56ms/step - loss: 0.0070 - binary_accuracy: 0.9989 - val_loss: 0.0119 - val_binary_accuracy: 0.9985\n","Epoch 7/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0050 - binary_accuracy: 0.9991 - val_loss: 0.0143 - val_binary_accuracy: 0.9984\n","Epoch 8/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0061 - binary_accuracy: 0.9991 - val_loss: 0.0109 - val_binary_accuracy: 0.9987\n","Epoch 9/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0059 - binary_accuracy: 0.9991 - val_loss: 0.0103 - val_binary_accuracy: 0.9987\n","Epoch 10/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0065 - binary_accuracy: 0.9991 - val_loss: 0.0160 - val_binary_accuracy: 0.9979\n","766/766 [==============================] - 13s 17ms/step - loss: 0.0160 - binary_accuracy: 0.9979\n","Loss: 0.01595223695039749\n","Accuracy: 0.9978765249252319\n","766/766 [==============================] - 13s 16ms/step\n","{'accuracy': 0.9978765109441359, 'precision': 0.9991166077738516, 'recall': 0.9951605807303123, 'f1_score': 0.997134670487106, 'tp': 2262, 'tn': 3847, 'fp': 2, 'fn': 11, 'feature_method': 'bert_en_cased_L-12_H-768_A-12', 'model': 'bert_en_cased_L-12_H-768_A-12', 'seed': 42, 'split_ratio': 0.2, 'train_size': 24487, 'test_size': 6122, 'extraction_time': 0, 'feature_size': 0, 'train_time': 70.81237946143949, 'pred_time': 2.125542135029905, 'dataset': 'SQLiV3.tsv'}\n","Appending to the existing .csv file.\n","****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\n","Epoch 1/10\n","3061/3061 [==============================] - 64s 20ms/step - loss: 0.3674 - binary_accuracy: 0.8214 - val_loss: 0.0945 - val_binary_accuracy: 0.9664\n","Epoch 2/10\n","3061/3061 [==============================] - 60s 20ms/step - loss: 0.0776 - binary_accuracy: 0.9780 - val_loss: 0.0565 - val_binary_accuracy: 0.9887\n","Epoch 3/10\n","3061/3061 [==============================] - 59s 19ms/step - loss: 0.0424 - binary_accuracy: 0.9902 - val_loss: 0.0342 - val_binary_accuracy: 0.9931\n","Epoch 4/10\n","3061/3061 [==============================] - 58s 19ms/step - loss: 0.0264 - binary_accuracy: 0.9940 - val_loss: 0.0197 - val_binary_accuracy: 0.9967\n","Epoch 5/10\n","3061/3061 [==============================] - 58s 19ms/step - loss: 0.0196 - binary_accuracy: 0.9955 - val_loss: 0.0187 - val_binary_accuracy: 0.9966\n","Epoch 6/10\n","3061/3061 [==============================] - 58s 19ms/step - loss: 0.0120 - binary_accuracy: 0.9973 - val_loss: 0.0159 - val_binary_accuracy: 0.9972\n","Epoch 7/10\n","3061/3061 [==============================] - 58s 19ms/step - loss: 0.0099 - binary_accuracy: 0.9979 - val_loss: 0.0174 - val_binary_accuracy: 0.9971\n","Epoch 8/10\n","3061/3061 [==============================] - 57s 19ms/step - loss: 0.0074 - binary_accuracy: 0.9983 - val_loss: 0.0175 - val_binary_accuracy: 0.9971\n","Epoch 9/10\n","3061/3061 [==============================] - 57s 19ms/step - loss: 0.0057 - binary_accuracy: 0.9988 - val_loss: 0.0164 - val_binary_accuracy: 0.9977\n","Epoch 10/10\n","3061/3061 [==============================] - 57s 19ms/step - loss: 0.0055 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 0.9975\n","766/766 [==============================] - 8s 10ms/step - loss: 0.0194 - binary_accuracy: 0.9975\n","Loss: 0.019398251548409462\n","Accuracy: 0.997549831867218\n","766/766 [==============================] - 8s 10ms/step\n","{'accuracy': 0.9977131656321464, 'precision': 1.0, 'recall': 0.9938407391113067, 'f1_score': 0.9969108561341571, 'tp': 2259, 'tn': 3849, 'fp': 0, 'fn': 14, 'feature_method': 'small_bert/bert_en_uncased_L-2_H-128_A-2', 'model': 'small_bert/bert_en_uncased_L-2_H-128_A-2', 'seed': 42, 'split_ratio': 0.2, 'train_size': 24487, 'test_size': 6122, 'extraction_time': 0, 'feature_size': 0, 'train_time': 23.92543859205683, 'pred_time': 1.298005539620869, 'dataset': 'SQLiV3.tsv'}\n","Appending to the existing .csv file.\n","****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n","Epoch 1/10\n","3061/3061 [==============================] - 84s 26ms/step - loss: 0.1825 - binary_accuracy: 0.9126 - val_loss: 0.0203 - val_binary_accuracy: 0.9961\n","Epoch 2/10\n","3061/3061 [==============================] - 80s 26ms/step - loss: 0.0163 - binary_accuracy: 0.9965 - val_loss: 0.0134 - val_binary_accuracy: 0.9977\n","Epoch 3/10\n","3061/3061 [==============================] - 80s 26ms/step - loss: 0.0097 - binary_accuracy: 0.9981 - val_loss: 0.0120 - val_binary_accuracy: 0.9980\n","Epoch 4/10\n","3061/3061 [==============================] - 81s 26ms/step - loss: 0.0078 - binary_accuracy: 0.9987 - val_loss: 0.0108 - val_binary_accuracy: 0.9982\n","Epoch 5/10\n","3061/3061 [==============================] - 80s 26ms/step - loss: 0.0061 - binary_accuracy: 0.9990 - val_loss: 0.0118 - val_binary_accuracy: 0.9982\n","Epoch 6/10\n","3061/3061 [==============================] - 79s 26ms/step - loss: 0.0036 - binary_accuracy: 0.9993 - val_loss: 0.0139 - val_binary_accuracy: 0.9985\n","Epoch 7/10\n","3061/3061 [==============================] - 79s 26ms/step - loss: 0.0046 - binary_accuracy: 0.9992 - val_loss: 0.0164 - val_binary_accuracy: 0.9980\n","Epoch 8/10\n","3061/3061 [==============================] - 77s 25ms/step - loss: 0.0054 - binary_accuracy: 0.9991 - val_loss: 0.0134 - val_binary_accuracy: 0.9987\n","Epoch 9/10\n","3061/3061 [==============================] - 77s 25ms/step - loss: 0.0037 - binary_accuracy: 0.9994 - val_loss: 0.0225 - val_binary_accuracy: 0.9980\n","Epoch 10/10\n","3061/3061 [==============================] - 76s 25ms/step - loss: 0.0030 - binary_accuracy: 0.9996 - val_loss: 0.0131 - val_binary_accuracy: 0.9984\n","766/766 [==============================] - 9s 12ms/step - loss: 0.0131 - binary_accuracy: 0.9984\n","Loss: 0.01308673806488514\n","Accuracy: 0.9983665347099304\n","766/766 [==============================] - 9s 11ms/step\n","{'accuracy': 0.9983665468801045, 'precision': 0.9995584988962473, 'recall': 0.9960404751429829, 'f1_score': 0.99779638607316, 'tp': 2264, 'tn': 3848, 'fp': 1, 'fn': 9, 'feature_method': 'small_bert/bert_en_uncased_L-4_H-512_A-8', 'model': 'small_bert/bert_en_uncased_L-4_H-512_A-8', 'seed': 42, 'split_ratio': 0.2, 'train_size': 24487, 'test_size': 6122, 'extraction_time': 0, 'feature_size': 0, 'train_time': 32.358936132687376, 'pred_time': 1.4509042857013392, 'dataset': 'SQLiV3.tsv'}\n","Appending to the existing .csv file.\n","****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1\n","Epoch 1/10\n","3061/3061 [==============================] - 120s 36ms/step - loss: 0.2908 - binary_accuracy: 0.8767 - val_loss: 0.0619 - val_binary_accuracy: 0.9877\n","Epoch 2/10\n","3061/3061 [==============================] - 108s 35ms/step - loss: 0.0435 - binary_accuracy: 0.9904 - val_loss: 0.0322 - val_binary_accuracy: 0.9953\n","Epoch 3/10\n","3061/3061 [==============================] - 109s 36ms/step - loss: 0.0234 - binary_accuracy: 0.9958 - val_loss: 0.0226 - val_binary_accuracy: 0.9962\n","Epoch 4/10\n","3061/3061 [==============================] - 109s 36ms/step - loss: 0.0150 - binary_accuracy: 0.9973 - val_loss: 0.0228 - val_binary_accuracy: 0.9969\n","Epoch 5/10\n","3061/3061 [==============================] - 108s 35ms/step - loss: 0.0125 - binary_accuracy: 0.9977 - val_loss: 0.0203 - val_binary_accuracy: 0.9971\n","Epoch 6/10\n","3061/3061 [==============================] - 109s 35ms/step - loss: 0.0094 - binary_accuracy: 0.9982 - val_loss: 0.0165 - val_binary_accuracy: 0.9982\n","Epoch 7/10\n","3061/3061 [==============================] - 108s 35ms/step - loss: 0.0088 - binary_accuracy: 0.9985 - val_loss: 0.0292 - val_binary_accuracy: 0.9961\n","Epoch 8/10\n","3061/3061 [==============================] - 108s 35ms/step - loss: 0.0068 - binary_accuracy: 0.9987 - val_loss: 0.0202 - val_binary_accuracy: 0.9974\n","Epoch 9/10\n","3061/3061 [==============================] - 109s 35ms/step - loss: 0.0056 - binary_accuracy: 0.9989 - val_loss: 0.0139 - val_binary_accuracy: 0.9985\n","Epoch 10/10\n","3061/3061 [==============================] - 111s 36ms/step - loss: 0.0057 - binary_accuracy: 0.9989 - val_loss: 0.0213 - val_binary_accuracy: 0.9977\n","766/766 [==============================] - 10s 13ms/step - loss: 0.0213 - binary_accuracy: 0.9977\n","Loss: 0.021299611777067184\n","Accuracy: 0.9977131485939026\n","766/766 [==============================] - 11s 13ms/step\n","{'accuracy': 0.9977131656321464, 'precision': 0.9982355535950596, 'recall': 0.9956005279366476, 'f1_score': 0.9969162995594713, 'tp': 2263, 'tn': 3845, 'fp': 4, 'fn': 10, 'feature_method': 'small_bert/bert_en_uncased_L-8_H-128_A-2', 'model': 'small_bert/bert_en_uncased_L-8_H-128_A-2', 'seed': 42, 'split_ratio': 0.2, 'train_size': 24487, 'test_size': 6122, 'extraction_time': 0, 'feature_size': 0, 'train_time': 44.88944070669369, 'pred_time': 1.7636456702979162, 'dataset': 'SQLiV3.tsv'}\n","Appending to the existing .csv file.\n","****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1\n","Epoch 1/10\n","3061/3061 [==============================] - 192s 57ms/step - loss: 0.1549 - binary_accuracy: 0.9253 - val_loss: 0.0193 - val_binary_accuracy: 0.9972\n","Epoch 2/10\n","3061/3061 [==============================] - 175s 57ms/step - loss: 0.0150 - binary_accuracy: 0.9968 - val_loss: 0.0146 - val_binary_accuracy: 0.9974\n","Epoch 3/10\n","3061/3061 [==============================] - 174s 57ms/step - loss: 0.0100 - binary_accuracy: 0.9980 - val_loss: 0.0180 - val_binary_accuracy: 0.9979\n","Epoch 4/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0075 - binary_accuracy: 0.9989 - val_loss: 0.0114 - val_binary_accuracy: 0.9987\n","Epoch 5/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0066 - binary_accuracy: 0.9989 - val_loss: 0.0148 - val_binary_accuracy: 0.9985\n","Epoch 6/10\n","3061/3061 [==============================] - 173s 57ms/step - loss: 0.0086 - binary_accuracy: 0.9987 - val_loss: 0.0134 - val_binary_accuracy: 0.9984\n","Epoch 7/10\n","3061/3061 [==============================] - 173s 57ms/step - loss: 0.0067 - binary_accuracy: 0.9990 - val_loss: 0.0113 - val_binary_accuracy: 0.9987\n","Epoch 8/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0072 - binary_accuracy: 0.9990 - val_loss: 0.0147 - val_binary_accuracy: 0.9984\n","Epoch 9/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0113 - binary_accuracy: 0.9982 - val_loss: 0.0188 - val_binary_accuracy: 0.9977\n","Epoch 10/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0828 - binary_accuracy: 0.9688 - val_loss: 0.0334 - val_binary_accuracy: 0.9943\n","766/766 [==============================] - 13s 17ms/step - loss: 0.0334 - binary_accuracy: 0.9943\n","Loss: 0.03335503488779068\n","Accuracy: 0.9942829012870789\n","766/766 [==============================] - 13s 15ms/step\n","{'accuracy': 0.9942829140803658, 'precision': 0.9995535714285714, 'recall': 0.9850417949846019, 'f1_score': 0.9922446266341679, 'tp': 2239, 'tn': 3848, 'fp': 1, 'fn': 34, 'feature_method': 'small_bert/bert_en_uncased_L-12_H-768_A-12', 'model': 'small_bert/bert_en_uncased_L-12_H-768_A-12', 'seed': 42, 'split_ratio': 0.2, 'train_size': 24487, 'test_size': 6122, 'extraction_time': 0, 'feature_size': 0, 'train_time': 71.34474909417055, 'pred_time': 2.102958419984246, 'dataset': 'SQLiV3.tsv'}\n","Appending to the existing .csv file.\n","****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\n","Epoch 1/10\n","3061/3061 [==============================] - 214s 64ms/step - loss: 0.1021 - binary_accuracy: 0.9522 - val_loss: 0.0190 - val_binary_accuracy: 0.9975\n","Epoch 2/10\n","3061/3061 [==============================] - 196s 64ms/step - loss: 0.0133 - binary_accuracy: 0.9976 - val_loss: 0.0165 - val_binary_accuracy: 0.9982\n","Epoch 3/10\n","3061/3061 [==============================] - 196s 64ms/step - loss: 0.0076 - binary_accuracy: 0.9987 - val_loss: 0.0166 - val_binary_accuracy: 0.9980\n","Epoch 4/10\n","3061/3061 [==============================] - 196s 64ms/step - loss: 0.0084 - binary_accuracy: 0.9987 - val_loss: 0.0176 - val_binary_accuracy: 0.9982\n","Epoch 5/10\n","3061/3061 [==============================] - 196s 64ms/step - loss: 0.0062 - binary_accuracy: 0.9991 - val_loss: 0.0143 - val_binary_accuracy: 0.9987\n","Epoch 6/10\n","3061/3061 [==============================] - 196s 64ms/step - loss: 0.0071 - binary_accuracy: 0.9991 - val_loss: 0.0260 - val_binary_accuracy: 0.9974\n","Epoch 7/10\n","3061/3061 [==============================] - 196s 64ms/step - loss: 0.0138 - binary_accuracy: 0.9980 - val_loss: 0.0142 - val_binary_accuracy: 0.9987\n","Epoch 8/10\n","3061/3061 [==============================] - 196s 64ms/step - loss: 0.0116 - binary_accuracy: 0.9984 - val_loss: 0.0172 - val_binary_accuracy: 0.9980\n","Epoch 9/10\n","3061/3061 [==============================] - 196s 64ms/step - loss: 0.0182 - binary_accuracy: 0.9971 - val_loss: 0.0139 - val_binary_accuracy: 0.9985\n","Epoch 10/10\n","3061/3061 [==============================] - 196s 64ms/step - loss: 0.0194 - binary_accuracy: 0.9949 - val_loss: 0.4291 - val_binary_accuracy: 0.9005\n","766/766 [==============================] - 13s 17ms/step - loss: 0.4291 - binary_accuracy: 0.9005\n","Loss: 0.4290808439254761\n","Accuracy: 0.9005227088928223\n","766/766 [==============================] - 13s 15ms/step\n","{'accuracy': 0.9005227049983665, 'precision': 0.7907058001397624, 'recall': 0.9956005279366476, 'f1_score': 0.8814021421616358, 'tp': 2263, 'tn': 3250, 'fp': 599, 'fn': 10, 'feature_method': 'bert_multi_cased_L-12_H-768_A-12', 'model': 'bert_multi_cased_L-12_H-768_A-12', 'seed': 42, 'split_ratio': 0.2, 'train_size': 24487, 'test_size': 6122, 'extraction_time': 0, 'feature_size': 0, 'train_time': 80.7081375904109, 'pred_time': 2.1087254468462353, 'dataset': 'SQLiV3.tsv'}\n","Appending to the existing .csv file.\n","****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/albert_en_base/2\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/albert_en_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/albert_en_base/2\n","Epoch 1/10\n","3061/3061 [==============================] - 175s 53ms/step - loss: 0.1648 - binary_accuracy: 0.9463 - val_loss: 0.0368 - val_binary_accuracy: 0.9951\n","Epoch 2/10\n","3061/3061 [==============================] - 163s 53ms/step - loss: 0.0415 - binary_accuracy: 0.9938 - val_loss: 0.0268 - val_binary_accuracy: 0.9966\n","Epoch 3/10\n","3061/3061 [==============================] - 161s 53ms/step - loss: 0.0240 - binary_accuracy: 0.9966 - val_loss: 0.0292 - val_binary_accuracy: 0.9961\n","Epoch 4/10\n","3061/3061 [==============================] - 163s 53ms/step - loss: 0.0256 - binary_accuracy: 0.9961 - val_loss: 0.0325 - val_binary_accuracy: 0.9962\n","Epoch 5/10\n","3061/3061 [==============================] - 164s 54ms/step - loss: 0.0342 - binary_accuracy: 0.9915 - val_loss: 0.0299 - val_binary_accuracy: 0.9966\n","Epoch 6/10\n","3061/3061 [==============================] - 164s 54ms/step - loss: 0.0223 - binary_accuracy: 0.9962 - val_loss: 0.0308 - val_binary_accuracy: 0.9966\n","Epoch 7/10\n","3061/3061 [==============================] - 165s 54ms/step - loss: 0.0221 - binary_accuracy: 0.9967 - val_loss: 0.0335 - val_binary_accuracy: 0.9958\n","Epoch 8/10\n","3061/3061 [==============================] - 165s 54ms/step - loss: 0.0406 - binary_accuracy: 0.9929 - val_loss: 0.0268 - val_binary_accuracy: 0.9964\n","Epoch 9/10\n","3061/3061 [==============================] - 164s 54ms/step - loss: 0.0566 - binary_accuracy: 0.9894 - val_loss: 0.0242 - val_binary_accuracy: 0.9969\n","Epoch 10/10\n","3061/3061 [==============================] - 164s 54ms/step - loss: 0.1145 - binary_accuracy: 0.9732 - val_loss: 0.0328 - val_binary_accuracy: 0.9941\n","766/766 [==============================] - 13s 17ms/step - loss: 0.0328 - binary_accuracy: 0.9941\n","Loss: 0.032751455903053284\n","Accuracy: 0.9941195845603943\n","766/766 [==============================] - 12s 15ms/step\n","{'accuracy': 0.9944462593923554, 'precision': 0.999553770638108, 'recall': 0.9854817421909371, 'f1_score': 0.9924678777137793, 'tp': 2240, 'tn': 3848, 'fp': 1, 'fn': 33, 'feature_method': 'albert_en_base', 'model': 'albert_en_base', 'seed': 42, 'split_ratio': 0.2, 'train_size': 24487, 'test_size': 6122, 'extraction_time': 0, 'feature_size': 0, 'train_time': 67.29904501290243, 'pred_time': 2.0442053368963164, 'dataset': 'SQLiV3.tsv'}\n","Appending to the existing .csv file.\n","****************************************************\n","BERT model selected           : https://tfhub.dev/google/electra_base/2\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","Training model with https://tfhub.dev/google/electra_base/2\n","Epoch 1/10\n","3061/3061 [==============================] - 191s 57ms/step - loss: 0.1563 - binary_accuracy: 0.9272 - val_loss: 0.0251 - val_binary_accuracy: 0.9964\n","Epoch 2/10\n","3061/3061 [==============================] - 173s 57ms/step - loss: 0.0183 - binary_accuracy: 0.9966 - val_loss: 0.0158 - val_binary_accuracy: 0.9979\n","Epoch 3/10\n","3061/3061 [==============================] - 173s 57ms/step - loss: 0.0140 - binary_accuracy: 0.9974 - val_loss: 0.0146 - val_binary_accuracy: 0.9977\n","Epoch 4/10\n","3061/3061 [==============================] - 173s 56ms/step - loss: 0.0130 - binary_accuracy: 0.9978 - val_loss: 0.0072 - val_binary_accuracy: 0.9989\n","Epoch 5/10\n","3061/3061 [==============================] - 173s 57ms/step - loss: 0.0089 - binary_accuracy: 0.9984 - val_loss: 0.0087 - val_binary_accuracy: 0.9989\n","Epoch 6/10\n","3061/3061 [==============================] - 174s 57ms/step - loss: 0.0079 - binary_accuracy: 0.9986 - val_loss: 0.0195 - val_binary_accuracy: 0.9977\n","Epoch 7/10\n","3061/3061 [==============================] - 173s 56ms/step - loss: 0.0096 - binary_accuracy: 0.9986 - val_loss: 0.0175 - val_binary_accuracy: 0.9984\n","Epoch 8/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0114 - binary_accuracy: 0.9983 - val_loss: 0.0160 - val_binary_accuracy: 0.9984\n","Epoch 9/10\n","3061/3061 [==============================] - 173s 56ms/step - loss: 0.0077 - binary_accuracy: 0.9989 - val_loss: 0.0106 - val_binary_accuracy: 0.9985\n","Epoch 10/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0100 - binary_accuracy: 0.9984 - val_loss: 0.0116 - val_binary_accuracy: 0.9985\n","766/766 [==============================] - 13s 17ms/step - loss: 0.0116 - binary_accuracy: 0.9985\n","Loss: 0.01163268182426691\n","Accuracy: 0.9985299110412598\n","766/766 [==============================] - 13s 15ms/step\n","{'accuracy': 0.9985298921920941, 'precision': 0.9991181657848325, 'recall': 0.9969203695556533, 'f1_score': 0.9980180576965426, 'tp': 2266, 'tn': 3847, 'fp': 2, 'fn': 7, 'feature_method': 'electra_base', 'model': 'electra_base', 'seed': 42, 'split_ratio': 0.2, 'train_size': 24487, 'test_size': 6122, 'extraction_time': 0, 'feature_size': 0, 'train_time': 71.34939285755333, 'pred_time': 2.104054981880973, 'dataset': 'SQLiV3.tsv'}\n","Appending to the existing .csv file.\n","****************************************************\n","BERT model selected           : https://tfhub.dev/google/electra_small/2\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","Training model with https://tfhub.dev/google/electra_small/2\n","Epoch 1/10\n","3061/3061 [==============================] - 163s 48ms/step - loss: 0.1702 - binary_accuracy: 0.9214 - val_loss: 0.0321 - val_binary_accuracy: 0.9944\n","Epoch 2/10\n","3061/3061 [==============================] - 145s 48ms/step - loss: 0.0211 - binary_accuracy: 0.9960 - val_loss: 0.0177 - val_binary_accuracy: 0.9971\n","Epoch 3/10\n","3061/3061 [==============================] - 145s 47ms/step - loss: 0.0146 - binary_accuracy: 0.9973 - val_loss: 0.0173 - val_binary_accuracy: 0.9972\n","Epoch 4/10\n","3061/3061 [==============================] - 146s 48ms/step - loss: 0.0101 - binary_accuracy: 0.9979 - val_loss: 0.0163 - val_binary_accuracy: 0.9979\n","Epoch 5/10\n","3061/3061 [==============================] - 145s 47ms/step - loss: 0.0083 - binary_accuracy: 0.9984 - val_loss: 0.0126 - val_binary_accuracy: 0.9980\n","Epoch 6/10\n","3061/3061 [==============================] - 145s 48ms/step - loss: 0.0069 - binary_accuracy: 0.9986 - val_loss: 0.0134 - val_binary_accuracy: 0.9984\n","Epoch 7/10\n","3061/3061 [==============================] - 145s 47ms/step - loss: 0.0065 - binary_accuracy: 0.9989 - val_loss: 0.0178 - val_binary_accuracy: 0.9980\n","Epoch 8/10\n","3061/3061 [==============================] - 144s 47ms/step - loss: 0.0084 - binary_accuracy: 0.9987 - val_loss: 0.0150 - val_binary_accuracy: 0.9980\n","Epoch 9/10\n","3061/3061 [==============================] - 144s 47ms/step - loss: 0.0053 - binary_accuracy: 0.9991 - val_loss: 0.0149 - val_binary_accuracy: 0.9980\n","Epoch 10/10\n","3061/3061 [==============================] - 145s 47ms/step - loss: 0.0035 - binary_accuracy: 0.9994 - val_loss: 0.0132 - val_binary_accuracy: 0.9985\n","766/766 [==============================] - 11s 15ms/step - loss: 0.0132 - binary_accuracy: 0.9985\n","Loss: 0.01322325598448515\n","Accuracy: 0.9985299110412598\n","766/766 [==============================] - 12s 14ms/step\n","{'accuracy': 0.9985298921920941, 'precision': 0.9991181657848325, 'recall': 0.9969203695556533, 'f1_score': 0.9980180576965426, 'tp': 2266, 'tn': 3847, 'fp': 2, 'fn': 7, 'feature_method': 'electra_small', 'model': 'electra_small', 'seed': 42, 'split_ratio': 0.2, 'train_size': 24487, 'test_size': 6122, 'extraction_time': 0, 'feature_size': 0, 'train_time': 59.942844436202456, 'pred_time': 1.9543311129360954, 'dataset': 'SQLiV3.tsv'}\n","Appending to the existing .csv file.\n"]}],"source":["for bert_model_name in bert_model_name_list:\n","  #bert_model_name = bert_model_name_list[0]\n","  print('****************************************************')\n","  #recording.set_current_method(f\"{bert_model_name}\" )\n","  tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","  tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","  print(f'BERT model selected           : {tfhub_handle_encoder}')\n","  print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n","\n","\n","  bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n","  bert_model = hub.KerasLayer(tfhub_handle_encoder)\n","  classifier_model = build_classifier_model(tfhub_handle_preprocess, tfhub_handle_encoder)\n","\n","  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","  metrics = tf.metrics.BinaryAccuracy()\n","\n","  epochs = 10 #modify this\n","  #steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n","  steps_per_epoch = data_manager.x_train.shape[0]\n","  num_train_steps = (steps_per_epoch * epochs) #//200 \n","  num_warmup_steps = int(0.1*num_train_steps)\n","\n","  init_lr = 3e-5\n","  optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                            num_train_steps=num_train_steps,\n","                                            num_warmup_steps=num_warmup_steps,\n","                                            optimizer_type='adamw')\n","\n","  classifier_model.compile(optimizer=optimizer,\n","                          loss=loss,\n","                          metrics=metrics)\n","\n","\n","  print(f'Training model with {tfhub_handle_encoder}')\n","  start_time = time.time()\n","  history = classifier_model.fit(x=train_ds, #train_ds.take(2)\n","                                validation_data=test_ds, # validation_data=test_ds.take(2),\n","                                epochs=epochs)\n","  stop_time = time.time()\n","  training_time = (stop_time - start_time)*1000 / data_manager.notes['train_size']\n","\n","  loss, accuracy = classifier_model.evaluate(test_ds)\n","\n","  print(f'Loss: {loss}')\n","  print(f'Accuracy: {accuracy}')\n","\n","  start_time = time.time()\n","  y_pred = classifier_model.predict(test_ds)\n","  stop_time = time.time()\n","  testing_time = (stop_time - start_time)*1000 / data_manager.notes['test_size']\n","  y_pred_np = tf.cast(tf.sigmoid(y_pred) > 0.5, tf.int32).numpy()\n","  X, y_true = zip(*test_ds.unbatch())\n","  y_true_np = [y.numpy() for y in y_true]\n","  #precision, recall, f1, support = precision_recall_fscore_support(y_true_np, y_pred_np, average='binary')\n","  #tn, fp, fn, tp = confusion_matrix(y_true_np, y_pred_np).ravel()\n","\n","  notes = {'feature_method': bert_model_name,'model': bert_model_name, \n","           'seed': data_manager.notes['seed'], \n","           'split_ratio': data_manager.notes['split_ratio'],\n","           'train_size': data_manager.notes['train_size'],\n","           'test_size': data_manager.notes['test_size'],\n","           'extraction_time':0, 'feature_size':0,\n","           'train_time': training_time,\n","           'pred_time': testing_time, 'dataset': config['dataset']['file']}\n","  result = evaluate(y_true_np, y_pred_np, notes)\n","  print(result)\n","  save_results([result], output_file)\n"]},{"cell_type":"markdown","metadata":{"id":"K0ObPxN_G4oS"},"source":["## The Proposed Cascade NLP (two-stage) SQLi Detection Tests"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":247,"status":"ok","timestamp":1681594360937,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"49AP1MiEnHz8"},"outputs":[],"source":["# Data manager _ split with seed\n","from sklearn.model_selection import train_test_split\n","\n","def split_dataset(seed=42):\n","  split_ratio = float(data_manager.config['data_manager']['split_ratio'])\n","\n","  data_manager.train, data_manager.test = train_test_split(\n","    data_manager.dataset, test_size=split_ratio, random_state=seed)\n","\n","  data_manager.x_train = data_manager.train['payload'].values\n","  data_manager.x_test = data_manager.test['payload'].values\n","  data_manager.y_train = data_manager.train['label'].values\n","  data_manager.y_test = data_manager.test['label'].values\n","\n","  data_manager.notes = {\n","    'seed': seed,\n","    'split_ratio': split_ratio,\n","    'train_size': len(data_manager.train),\n","    'test_size': len(data_manager.test),\n","  }\n","\n","#split_dataset(seed=666)\n","#print(data_manager.notes['seed'])"]},{"cell_type":"markdown","metadata":{"id":"_nC5YnC-ULNx"},"source":["### First Stage - Classical ML"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":230,"status":"ok","timestamp":1681594365128,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"kLOa8sfNqfvw"},"outputs":[],"source":["from templates import FeatureExtractor\n","from classical_models import Classical_Model\n","from pprint import pprint\n","from experiments import evaluate\n","\n","threshold = 0.05 #prediction\n","scale_pos_weight = 5000.0\n","\n","feature_method = 'tf-idf_ngram'\n","model_name = 'xgboost'"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":424,"status":"ok","timestamp":1681594812074,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"SeqeDDr8HCVt"},"outputs":[],"source":["# Train and evaluate the first stage\n","def first_stage():\n","  feature_extractor = FeatureExtractor(feature_method)\n","  start_time = time.time()\n","  feature_extractor.extract_features(\n","      data_manager.x_train, data_manager.x_test)\n","  stop_time = time.time()\n","  extraction_time = ((stop_time - start_time)*1000 \n","                    / (len(data_manager.x_train) + len(data_manager.x_test)) )\n","\n","  model = Classical_Model(model_name, scale_pos_weight=scale_pos_weight)\n","  model.feature_method = feature_extractor.method\n","  start_time = time.time()\n","  model.fit(\n","      feature_extractor.features['train'], \n","      data_manager.y_train) #scale_pos_weight=5.0\n","  stop_time = time.time()\n","  training_time = (stop_time - start_time)*1000 / feature_extractor.features['train'].shape[0]\n","\n","\n","  start_time = time.time()\n","  first_stage_y_pred = model.predict(feature_extractor.features['test'], threshold=threshold)\n","  stop_time = time.time()\n","\n","  testing_time = (stop_time - start_time)*1000 / feature_extractor.features['test'].shape[0]\n","\n","  notes = {'feature_method': feature_extractor.method,'model': model_name, \n","            'seed': data_manager.notes['seed'], \n","            'split_ratio': data_manager.notes['split_ratio'],\n","            'train_size': data_manager.notes['train_size'],\n","            'test_size': data_manager.notes['test_size'],\n","            'extraction_time':extraction_time, 'feature_size': feature_extractor.features['train'].shape[1],\n","            'train_time': training_time,\n","            'pred_time': testing_time, 'dataset': config['dataset']['file'],\n","          'scale_pos_weight':scale_pos_weight,\n","          'threshold': threshold\n","          }\n","  # Save results to csv file\n","\n","  result = evaluate(data_manager.y_test, first_stage_y_pred, notes=notes)\n","  pprint(result)\n","  save_results([result], proposed_test_results_file)\n","\n","  # Extract the positive predicitions for the second stage\n","  first_stage_positive_preds = data_manager.x_test[(first_stage_y_pred == 1)]\n","  first_stage_positive_preds_true_labels = data_manager.y_test[(first_stage_y_pred == 1)]\n","  fs_pos_len = len(first_stage_positive_preds)\n","  nof_test_samples = len(data_manager.x_test)\n","  fs_rat = fs_pos_len/nof_test_samples\n","  print(f\"Positive predicitions in the first stage: {fs_pos_len} out of {nof_test_samples}. Ratio:{fs_rat}\")\n","\n","  return first_stage_positive_preds, first_stage_positive_preds_true_labels"]},{"cell_type":"markdown","metadata":{"id":"Xg20dUuRWJ-4"},"source":["### Second Stage - BERT"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":233,"status":"ok","timestamp":1681595398871,"user":{"displayName":"Kasim Tasdemir","userId":"13577932980440881494"},"user_tz":-60},"id":"WrftzeG9XwOx"},"outputs":[],"source":["def second_stage(first_stage_positive_preds, first_stage_positive_preds_true_labels):\n","  # Prepare the training Dataset \n","  currentDateAndTime = datetime.now()\n","  currentTime = currentDateAndTime.strftime(\"%y%m%d_%H%M%S\")\n","  #recording = Recorder(main_folder / results_dir / f'results_BERT_{currentTime}.csv')\n","\n","\n","  AUTOTUNE = tf.data.AUTOTUNE\n","  batch_size = 8\n","  #seed = 42\n","\n","  x_train = data_manager.x_train\n","  y_train = data_manager.y_train\n","  x_test = data_manager.x_test\n","  y_test = data_manager.y_test\n","\n","  # Create TensorFlow datasets for the training and validation sets\n","  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n","  val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n","\n","  # Cache the training and validation datasets\n","  train_dataset = train_dataset.cache()\n","  val_dataset = val_dataset.cache()\n","\n","\n","  # Prefetch the training and validation datasets\n","  train_ds = train_dataset.prefetch(buffer_size=AUTOTUNE)\n","  test_ds = val_dataset.prefetch(buffer_size=AUTOTUNE)\n","\n","  class_names = ['payload','label']\n","\n","  # First, train the BERT model.\n","\n","  print('****************************************************')\n","  bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n","  tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","  tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","  print(f'BERT model selected           : {tfhub_handle_encoder}')\n","  print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n","\n","\n","  bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n","  bert_model = hub.KerasLayer(tfhub_handle_encoder)\n","  second_stage_classifier_model = build_classifier_model(tfhub_handle_preprocess, tfhub_handle_encoder)\n","\n","  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","  metrics = tf.metrics.BinaryAccuracy()\n","\n","  epochs = 10 #DEBUG MODIFY ME\n","  #steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n","  steps_per_epoch = data_manager.x_train.shape[0]\n","  num_train_steps = (steps_per_epoch * epochs) #//200 \n","  num_warmup_steps = int(0.1*num_train_steps)\n","\n","  init_lr = 3e-5\n","  optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                            num_train_steps=num_train_steps,\n","                                            num_warmup_steps=num_warmup_steps,\n","                                            optimizer_type='adamw')\n","\n","  second_stage_classifier_model.compile(optimizer=optimizer,\n","                          loss=loss,\n","                          metrics=metrics)\n","\n","\n","  print(f'Training model with {tfhub_handle_encoder}')\n","  start_time = time.time()\n","  history = second_stage_classifier_model.fit(x=train_ds, # x=train_ds.take(2), \n","                                validation_data=test_ds, # validation_data=test_ds.take(2), \n","                                epochs=epochs)\n","  stop_time = time.time()\n","  training_time = (stop_time - start_time)*1000 / data_manager.notes['train_size']\n","\n","  # Prepare first stage outputs for the second stage\n","  print(len(first_stage_positive_preds)) #np array\n","  print(len(first_stage_positive_preds_true_labels)) # np array\n","\n","  # Create TensorFlow datasets for the prediction set\n","  pred_dataset = tf.data.Dataset.from_tensor_slices(\n","      (first_stage_positive_preds, first_stage_positive_preds_true_labels)).batch(batch_size)\n","\n","  # Cache the pred dataset\n","  pred_dataset = pred_dataset.cache()\n","\n","  # Prefetch the pred datasets\n","  first_stage_pos_preds = pred_dataset.prefetch(buffer_size=AUTOTUNE)\n","\n","  # Predict only positive outputs of the first stage\n","  start_time = time.time()\n","  y_pred = second_stage_classifier_model.predict(first_stage_pos_preds)#.take(2))\n","  stop_time = time.time()\n","  testing_time = (stop_time - start_time)*1000 / len(first_stage_positive_preds) \n","  y_pred_np = tf.cast(tf.sigmoid(y_pred) > 0.5, tf.int32).numpy()\n","  X, y_true = zip(*first_stage_pos_preds.unbatch())# take(2).\n","  y_true_np = [y.numpy() for y in y_true]\n","  #precision, recall, f1, support = precision_recall_fscore_support(y_true_np, y_pred_np, average='binary')\n","  #tn, fp, fn, tp = confusion_matrix(y_true_np, y_pred_np).ravel()\n","\n","  notes = {'feature_method': bert_model_name,'model': bert_model_name, \n","            'seed': data_manager.notes['seed'], \n","            'split_ratio': data_manager.notes['split_ratio'],\n","            'train_size': data_manager.notes['train_size'],\n","            'test_size': data_manager.notes['test_size'],\n","            'extraction_time':0, 'feature_size':0,\n","            'train_time': training_time,\n","            'pred_time': testing_time, 'dataset': config['dataset']['file']}\n","  result = evaluate(y_true_np, y_pred_np, notes)\n","  pprint(result)\n","  save_results([result], proposed_test_results_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqfbUsmjrhSB","outputId":"8d0b87d3-f1c2-4f63-d37a-16a9f3c798bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["######################################\n","seed:23\n","23\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["{'accuracy': 0.9862789937928781,\n"," 'dataset': 'SQLiV3.tsv',\n"," 'extraction_time': 0.08238786935860798,\n"," 'f1_score': 0.9819742489270387,\n"," 'feature_method': 'tf-idf_ngram',\n"," 'feature_size': 29074,\n"," 'fn': 3,\n"," 'fp': 81,\n"," 'model': 'xgboost',\n"," 'precision': 0.9658083579569439,\n"," 'pred_time': 0.002254227803681108,\n"," 'recall': 0.9986905281536447,\n"," 'scale_pos_weight': 5000.0,\n"," 'seed': 23,\n"," 'split_ratio': 0.2,\n"," 'test_size': 6122,\n"," 'threshold': 0.05,\n"," 'tn': 3750,\n"," 'tp': 2288,\n"," 'train_size': 24487,\n"," 'train_time': 0.577415710968293}\n","Positive predicitions in the first stage: 2369 out of 6122. Ratio:0.38696504410323423\n","****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Epoch 1/10\n","3061/3061 [==============================] - 188s 56ms/step - loss: 0.1328 - binary_accuracy: 0.9411 - val_loss: 0.0220 - val_binary_accuracy: 0.9961\n","Epoch 2/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0127 - binary_accuracy: 0.9976 - val_loss: 0.0378 - val_binary_accuracy: 0.9940\n","Epoch 3/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0092 - binary_accuracy: 0.9984 - val_loss: 0.0110 - val_binary_accuracy: 0.9985\n","Epoch 4/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0066 - binary_accuracy: 0.9989 - val_loss: 0.0245 - val_binary_accuracy: 0.9975\n","Epoch 5/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0066 - binary_accuracy: 0.9990 - val_loss: 0.0201 - val_binary_accuracy: 0.9972\n","Epoch 6/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0066 - binary_accuracy: 0.9989 - val_loss: 0.0222 - val_binary_accuracy: 0.9975\n","Epoch 7/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0039 - binary_accuracy: 0.9994 - val_loss: 0.0347 - val_binary_accuracy: 0.9959\n","Epoch 8/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0082 - binary_accuracy: 0.9988 - val_loss: 0.0207 - val_binary_accuracy: 0.9971\n","Epoch 9/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0059 - binary_accuracy: 0.9991 - val_loss: 0.0238 - val_binary_accuracy: 0.9961\n","Epoch 10/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0079 - binary_accuracy: 0.9987 - val_loss: 0.0159 - val_binary_accuracy: 0.9975\n","2369\n","2369\n","297/297 [==============================] - 5s 15ms/step\n","{'accuracy': 0.9957788096243141,\n"," 'dataset': 'SQLiV3.tsv',\n"," 'extraction_time': 0,\n"," 'f1_score': 0.9978108581436078,\n"," 'feature_method': 'bert_en_uncased_L-12_H-768_A-12',\n"," 'feature_size': 0,\n"," 'fn': 9,\n"," 'fp': 1,\n"," 'model': 'bert_en_uncased_L-12_H-768_A-12',\n"," 'precision': 0.999561403508772,\n"," 'pred_time': 2.3272827538119536,\n"," 'recall': 0.9960664335664335,\n"," 'seed': 23,\n"," 'split_ratio': 0.2,\n"," 'test_size': 6122,\n"," 'tn': 80,\n"," 'tp': 2279,\n"," 'train_size': 24487,\n"," 'train_time': 70.61641637967489}\n","Appending to the existing .csv file.\n","######################################\n","seed:6\n","6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["{'accuracy': 0.9851355766089513,\n"," 'dataset': 'SQLiV3.tsv',\n"," 'extraction_time': 0.0834205024497652,\n"," 'f1_score': 0.9805264284185747,\n"," 'feature_method': 'tf-idf_ngram',\n"," 'feature_size': 29196,\n"," 'fn': 6,\n"," 'fp': 85,\n"," 'model': 'xgboost',\n"," 'precision': 0.9642255892255892,\n"," 'pred_time': 0.0019423987342812041,\n"," 'recall': 0.9973878972572922,\n"," 'scale_pos_weight': 5000.0,\n"," 'seed': 6,\n"," 'split_ratio': 0.2,\n"," 'test_size': 6122,\n"," 'threshold': 0.05,\n"," 'tn': 3740,\n"," 'tp': 2291,\n"," 'train_size': 24487,\n"," 'train_time': 0.48259850076449606}\n","Appending to the existing .csv file.\n","Positive predicitions in the first stage: 2376 out of 6122. Ratio:0.38810846128716103\n","****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Epoch 1/10\n","3061/3061 [==============================] - 189s 56ms/step - loss: 0.1779 - binary_accuracy: 0.9182 - val_loss: 0.0173 - val_binary_accuracy: 0.9969\n","Epoch 2/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0131 - binary_accuracy: 0.9975 - val_loss: 0.0177 - val_binary_accuracy: 0.9977\n","Epoch 3/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0116 - binary_accuracy: 0.9979 - val_loss: 0.0098 - val_binary_accuracy: 0.9980\n","Epoch 4/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0067 - binary_accuracy: 0.9989 - val_loss: 0.0102 - val_binary_accuracy: 0.9984\n","Epoch 5/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0065 - binary_accuracy: 0.9987 - val_loss: 0.0155 - val_binary_accuracy: 0.9979\n","Epoch 6/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0068 - binary_accuracy: 0.9988 - val_loss: 0.0090 - val_binary_accuracy: 0.9985\n","Epoch 7/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0045 - binary_accuracy: 0.9993 - val_loss: 0.0141 - val_binary_accuracy: 0.9985\n","Epoch 8/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0106 - binary_accuracy: 0.9985 - val_loss: 0.0169 - val_binary_accuracy: 0.9984\n","Epoch 9/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0063 - binary_accuracy: 0.9990 - val_loss: 0.0098 - val_binary_accuracy: 0.9984\n","Epoch 10/10\n","3061/3061 [==============================] - 171s 56ms/step - loss: 0.0043 - binary_accuracy: 0.9992 - val_loss: 0.0109 - val_binary_accuracy: 0.9985\n","2376\n","2376\n","297/297 [==============================] - 6s 16ms/step\n","{'accuracy': 0.9974747474747475,\n"," 'dataset': 'SQLiV3.tsv',\n"," 'extraction_time': 0,\n"," 'f1_score': 0.9986905281536447,\n"," 'feature_method': 'bert_en_uncased_L-12_H-768_A-12',\n"," 'feature_size': 0,\n"," 'fn': 3,\n"," 'fp': 3,\n"," 'model': 'bert_en_uncased_L-12_H-768_A-12',\n"," 'precision': 0.9986905281536447,\n"," 'pred_time': 2.3657764850642145,\n"," 'recall': 0.9986905281536447,\n"," 'seed': 6,\n"," 'split_ratio': 0.2,\n"," 'test_size': 6122,\n"," 'tn': 82,\n"," 'tp': 2288,\n"," 'train_size': 24487,\n"," 'train_time': 70.62725064295603}\n","Appending to the existing .csv file.\n","######################################\n","seed:34\n","34\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["{'accuracy': 0.9857889578569095,\n"," 'dataset': 'SQLiV3.tsv',\n"," 'extraction_time': 0.0829493046476753,\n"," 'f1_score': 0.9810992830762545,\n"," 'feature_method': 'tf-idf_ngram',\n"," 'feature_size': 27216,\n"," 'fn': 4,\n"," 'fp': 83,\n"," 'model': 'xgboost',\n"," 'precision': 0.964545066211021,\n"," 'pred_time': 0.004984124428849717,\n"," 'recall': 0.9982316534040672,\n"," 'scale_pos_weight': 5000.0,\n"," 'seed': 34,\n"," 'split_ratio': 0.2,\n"," 'test_size': 6122,\n"," 'threshold': 0.05,\n"," 'tn': 3777,\n"," 'tp': 2258,\n"," 'train_size': 24487,\n"," 'train_time': 0.5836119651599703}\n","Appending to the existing .csv file.\n","Positive predicitions in the first stage: 2341 out of 6122. Ratio:0.38239137536752693\n","****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Epoch 1/10\n","3061/3061 [==============================] - 188s 56ms/step - loss: 0.1518 - binary_accuracy: 0.9260 - val_loss: 0.0146 - val_binary_accuracy: 0.9975\n","Epoch 2/10\n","3061/3061 [==============================] - 170s 55ms/step - loss: 0.0142 - binary_accuracy: 0.9973 - val_loss: 0.0113 - val_binary_accuracy: 0.9980\n","Epoch 3/10\n","3061/3061 [==============================] - 168s 55ms/step - loss: 0.0098 - binary_accuracy: 0.9985 - val_loss: 0.0095 - val_binary_accuracy: 0.9980\n","Epoch 4/10\n","3061/3061 [==============================] - 169s 55ms/step - loss: 0.0054 - binary_accuracy: 0.9991 - val_loss: 0.0116 - val_binary_accuracy: 0.9985\n","Epoch 5/10\n","3061/3061 [==============================] - 168s 55ms/step - loss: 0.0066 - binary_accuracy: 0.9990 - val_loss: 0.0111 - val_binary_accuracy: 0.9974\n","Epoch 6/10\n","3061/3061 [==============================] - 168s 55ms/step - loss: 0.0072 - binary_accuracy: 0.9989 - val_loss: 0.0107 - val_binary_accuracy: 0.9989\n","Epoch 7/10\n","3061/3061 [==============================] - 168s 55ms/step - loss: 0.0061 - binary_accuracy: 0.9991 - val_loss: 0.0117 - val_binary_accuracy: 0.9982\n","Epoch 8/10\n","3061/3061 [==============================] - 168s 55ms/step - loss: 0.0066 - binary_accuracy: 0.9991 - val_loss: 0.0131 - val_binary_accuracy: 0.9984\n","Epoch 9/10\n","3061/3061 [==============================] - 167s 54ms/step - loss: 0.0059 - binary_accuracy: 0.9991 - val_loss: 0.0105 - val_binary_accuracy: 0.9982\n","Epoch 10/10\n","3061/3061 [==============================] - 166s 54ms/step - loss: 0.0029 - binary_accuracy: 0.9996 - val_loss: 0.0150 - val_binary_accuracy: 0.9987\n","2341\n","2341\n","293/293 [==============================] - 5s 15ms/step\n","{'accuracy': 0.9970098248611704,\n"," 'dataset': 'SQLiV3.tsv',\n"," 'extraction_time': 0,\n"," 'f1_score': 0.9984475493457529,\n"," 'feature_method': 'bert_en_uncased_L-12_H-768_A-12',\n"," 'feature_size': 0,\n"," 'fn': 7,\n"," 'fp': 0,\n"," 'model': 'bert_en_uncased_L-12_H-768_A-12',\n"," 'precision': 1.0,\n"," 'pred_time': 2.3221387052474904,\n"," 'recall': 0.9968999114260407,\n"," 'seed': 34,\n"," 'split_ratio': 0.2,\n"," 'test_size': 6122,\n"," 'tn': 83,\n"," 'tp': 2251,\n"," 'train_size': 24487,\n"," 'train_time': 69.38682902500435}\n","Appending to the existing .csv file.\n","######################################\n","seed:1984\n","1984\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["{'accuracy': 0.9872590656648155,\n"," 'dataset': 'SQLiV3.tsv',\n"," 'extraction_time': 0.08288315905083352,\n"," 'f1_score': 0.9830065359477125,\n"," 'feature_method': 'tf-idf_ngram',\n"," 'feature_size': 29147,\n"," 'fn': 4,\n"," 'fp': 74,\n"," 'model': 'xgboost',\n"," 'precision': 0.9682403433476395,\n"," 'pred_time': 0.0021736125703030883,\n"," 'recall': 0.9982300884955753,\n"," 'scale_pos_weight': 5000.0,\n"," 'seed': 1984,\n"," 'split_ratio': 0.2,\n"," 'test_size': 6122,\n"," 'threshold': 0.05,\n"," 'tn': 3788,\n"," 'tp': 2256,\n"," 'train_size': 24487,\n"," 'train_time': 0.5747240745515457}\n","Appending to the existing .csv file.\n","Positive predicitions in the first stage: 2330 out of 6122. Ratio:0.38059457693564197\n","****************************************************\n","BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","Training model with https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Epoch 1/10\n","3061/3061 [==============================] - 184s 55ms/step - loss: 0.1468 - binary_accuracy: 0.9378 - val_loss: 0.0157 - val_binary_accuracy: 0.9967\n","Epoch 2/10\n","3061/3061 [==============================] - 170s 56ms/step - loss: 0.0146 - binary_accuracy: 0.9974 - val_loss: 0.0090 - val_binary_accuracy: 0.9985\n","Epoch 3/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0076 - binary_accuracy: 0.9987 - val_loss: 0.0105 - val_binary_accuracy: 0.9987\n","Epoch 4/10\n","3061/3061 [==============================] - 172s 56ms/step - loss: 0.0089 - binary_accuracy: 0.9983 - val_loss: 0.0061 - val_binary_accuracy: 0.9989\n","Epoch 5/10\n","1217/3061 [==========>...................] - ETA: 1:36 - loss: 0.0105 - binary_accuracy: 0.9985"]}],"source":["# Run first and second stages with different seeds:\n","seeds = [23, 6, 34, 1984, 1994, 77]\n","for seed in seeds:\n","  print('######################################')\n","  print(f\"seed:{seed}\")\n","  split_dataset(seed=seed)\n","  print(data_manager.notes['seed'])\n","  first_stage_y_pred, first_stage_positive_preds_true_labels = first_stage()\n","  second_stage(first_stage_y_pred, first_stage_positive_preds_true_labels)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb","timestamp":1675862264150}],"toc_visible":true},"gpuClass":"premium","kernelspec":{"display_name":"rafi-sqli","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"vscode":{"interpreter":{"hash":"085c9640c77a34c1b406144a3bd4a3c83460825e7fcd209668d2630032a9442a"}}},"nbformat":4,"nbformat_minor":0}