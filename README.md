# Hybrid Features Extraction Approach using Natural Language Processing for Improved Detection of SQL Injection Attack

This is the implementation of "Hybrid Features Extraction Approach using Natural Language Processing for Improved Detection of SQL Injection Attack" algorithm. It can load the custom datasets, train various models and demonstrate their inference performances. 
## Requirements  

- (recommended) [Mambaforge](https://github.com/conda-forge/miniforge#mambaforge). Mamba package search is significantly faster than Anaconda (or [Anaconda](https://www.anaconda.com/products/distribution) environment)
- Google Colab account, or any other Jupyter Notebook with GPU support.
## Setup
The first part (Classical ML methods) can run on virtual Python environment on Win, Mac or Linux.

- Download and install the [Mambaforge](https://github.com/conda-forge/miniforge#mambaforge) environment.
- Open Miniforge Prompt and change current directory to the project folder. 
- (Optional) If you have GPU and want to use it update the yml file to contain "tensorflow-gpu".(this might not work. Not tested)
- Run the following command in the folder, where **rafi-sqli.yml** file resides. This will create a new Python environment with the required packages:
    -  ``` mamba env create -f rafi-sqli.yml ```
- Activate the environment
    - ``` mamba activate rafi-sqli ```
- Install the following packages:
    - ``` pip install -q -U "tensorflow-text==2.8.*" ```
    - ``` pip install -q tf-models-official==2.7.0 ```


## Running
- Make sure that the *train* and *test* datasets are in the Dataset folder, and conf/nlp.conf file is modified accordingly.

Classical ML methods work in Mamba environment without GPU support. Bert models can run in Google Colab. They need to use GPU. For that reason there are classical ML methods and BERT models are tested separately. Then the results are merged. The following steps explains how to run all tests.

### Classical ML based methods
- Activate rafi-sqli environment in the Miniforge prompt and run:
    - ``` python nlp_hybrid.py ```

- Finally, to run all tests by (or type *help* for other commands but not every command is tested):
    - ``` all ```

### BERT models
- Upload and open ``` package/detect_sqi_with_transformers.ipynb ```  in Google Colab with GPU support.
- Upload the datasets to next to the file.
- Run the all cells

### Combining and displaying the experimental results

- Put the results*.pkl files generated by BERT models and Classical ML methods into the main folder (where nlp_hybrid.py resides).
- In rafi-sqli environment, run
  - ``` jupyter notebook ```
- Choose ``` utils\Demonstrate_test_results.ipynb ``` file in the Jupyter Notebook.
- After updating the result*.pkl file paths, run the all code.

## Folder contents
- TBA
- Main folders: (package, datasets, utils, conf)
  - files: *.pkl files are generated by tests. Some of them contains DS1, DS2 results. See "demonstrate test results file to see the contents of them"
  - conf: contains the config file for classical MLs (not BERT, jupyternotebook)
  - package: python functions of the models, training, testing, etc.
  - utils   
    - kaggle dataset cleaning code,
    - convert Rafi's old dataset structure to new one (delimiter is changed)
    - split the full dataset into train and test pairs
    - Demonstrate test results : produce all visuals and tables used in the paper.
    - utils\Demonstrate_test_results_HARIKAonly.ipynb visualize the test results on Harika's DS only. For kaggle, see utils\Demonstrate_test_results.ipynb
## Troubleshot

- Only 'all' command might work properly. The others have not been tested after modifications.
## Release notes
- Release (version TBA)
  - 
- Release (0.3.0) Latex table generation
  - The results are saved to a pkl file in the main folder.
  - the results pickle files can be read and visualized using utils\Demonstrate_test_results.ipynb. This also generates the Latex tables used in the paper.
  - the visualized results have color scheme. The fonts sizes, etc. are ready for the paper.
- Release (v0.2.0)
    - BERT is added to the tests. See ``` package/detect_sqi_with_transformers.ipynb ```
    - ``` utils\Demonstrate_test_results.ipynb ``` combine the results and demonstrates the experimental results.
- Release (v0.1.0)
    - Results are saved to a Pandas dataframe. It is saved to a pickle file.
    - Results can be visualized using Utils/Data visualize . jpy notebook.
    - The original code required datasets with 'delimiter=three tabs'. This is no longer supported by Pandas data frame. So, It has been changed to support single tab delimited dataset. If you need to use the old code on the old datasets, you can use Release v0.0.12
	- utils/clean-kaggle-sqli-dataset-and-split.ipynb file is created for cleaning Kaggle SQLi dataset and splitting it into train-test files.
	- utils/convert-old-dataset-to-new-single-tab.ipynb file is created for converting old three tabs delimited dataset to single tab.
	- utils/dataset-train-test-splitter.ipynb file is created for splitting the given dataset to train and test parts.
	- support for running without building the package is added (nlp_hybrid.py)

- Release v0.0.12

    - This is the original , the first code from Rafi. It works with Python 3 (the very first one was Python 2)


 
